const OpenAI = require('openai');
const { Pinecone } = require('@pinecone-database/pinecone');
const fs = require('fs');
const path = require('path');
const mammoth = require('mammoth');
const cheerio = require('cheerio');
const pdfParsingService = require('./pdfParsingService');
const openaiManager = require('./openaiManager');

class SimpleDocumentProcessor {
  constructor() {
    this.openai = null;
    this.pinecone = null;
    this.isReady = false;
    this.init();
  }

  async init() {
    try {
      // Initialize OpenAI Manager
      await openaiManager.initialize();
      this.openai = openaiManager.client;

      // Initialize Pinecone
      this.pinecone = new Pinecone({
        apiKey: process.env.PINECONE_API_KEY
      });

      this.isReady = true;
      console.log('‚úÖ Document Processor Ready with enhanced OpenAI integration');
    } catch (error) {
      console.error('‚ùå Document Processor Init Failed:', error.message);
      this.isReady = false;
    }
  }

  async processDocument(filePath, userPineconeId, metadata = {}) {
    if (!this.isReady) {
      throw new Error('Document processor not ready');
    }

    try {
      console.log(`üìÑ Processing: ${path.basename(filePath)}`);

      // 1. Extract text from file
      const text = await this.extractText(filePath);
      
      // 2. Split into chunks
      const chunks = this.splitIntoChunks(text, 500); // 500 char chunks
      
      // 3. Process each chunk
      let stored = 0;
      for (let i = 0; i < chunks.length; i++) {
        const chunk = chunks[i];
        if (chunk.trim().length < 10) continue; // Skip tiny chunks
        
        // Create embedding
        const embedding = await this.createEmbedding(chunk);
        
        // Store in Pinecone
        await this.storeChunk(userPineconeId, {
          id: `${metadata.fileName}_chunk_${i}`,
          values: embedding,
          metadata: {
            fileName: metadata.fileName || path.basename(filePath),
            fileType: metadata.fileType || 'unknown',
            text: chunk,
            chunkIndex: i,
            uploadDate: new Date().toISOString()
          }
        });
        
        stored++;
      }

      console.log(`‚úÖ Processed ${chunks.length} chunks, stored ${stored}`);
      return { success: true, chunksProcessed: chunks.length, chunksStored: stored };

    } catch (error) {
      console.error(`‚ùå Processing failed for ${filePath}:`, error.message);
      throw error;
    }
  }

  async extractText(filePath) {
    const ext = path.extname(filePath).toLowerCase();
    const buffer = fs.readFileSync(filePath);

    switch (ext) {
      case '.pdf':
        return await pdfParsingService.extractText(buffer);
      case '.docx':
        const result = await mammoth.extractRawText({ buffer });
        return result.value;
      case '.html':
      case '.htm':
        const $ = cheerio.load(buffer.toString());
        $('script, style').remove();
        return $.text();
      case '.txt':
        return buffer.toString('utf-8');
      default:
        throw new Error(`Unsupported file type: ${ext}`);
    }
  }

  splitIntoChunks(text, chunkSize = 500) {
    const chunks = [];
    const sentences = text.split(/[.!?]+/).filter(s => s.trim().length > 0);
    
    let currentChunk = '';
    
    for (const sentence of sentences) {
      if (currentChunk.length + sentence.length > chunkSize && currentChunk.length > 0) {
        chunks.push(currentChunk.trim());
        currentChunk = sentence.trim();
      } else {
        currentChunk += (currentChunk ? '. ' : '') + sentence.trim();
      }
    }
    
    if (currentChunk.trim()) {
      chunks.push(currentChunk.trim());
    }
    
    return chunks;
  }

  async createEmbedding(text) {
    try {
      const result = await openaiManager.createEmbedding(text);
      return result.embedding;
      
    } catch (error) {
      console.error('‚ùå OpenAI Embedding creation failed:', error.message);
      
      // Enhanced fallback embedding system
      return this.createFallbackEmbedding(text);
    }
  }

  createFallbackEmbedding(text) {
    console.log('üîÑ Using fallback embedding for document processing');
    
    const words = text.toLowerCase().split(/\s+/);
    const embedding = new Array(384).fill(0);
    
    // Create a more sophisticated embedding based on word patterns
    words.forEach((word, wordIndex) => {
      // Skip very short words
      if (word.length < 2) return;
      
      // Create multiple hash positions for each word
      for (let i = 0; i < Math.min(word.length, 10); i++) {
        const charCode = word.charCodeAt(i);
        const position1 = (wordIndex * 37 + i * 17 + charCode) % 384;
        const position2 = (wordIndex * 23 + i * 13 + charCode * 7) % 384;
        
        embedding[position1] += (charCode / 255) - 0.5;
        embedding[position2] += (charCode / 127) - 1.0;
      }
      
      // Add word length and position information
      const lengthPos = (word.length * 31 + wordIndex * 7) % 384;
      embedding[lengthPos] += word.length / 20.0;
    });
    
    // Normalize the embedding vector
    const magnitude = Math.sqrt(embedding.reduce((sum, val) => sum + val * val, 0));
    if (magnitude > 0) {
      for (let i = 0; i < embedding.length; i++) {
        embedding[i] /= magnitude;
      }
    }
    
    return embedding;
  }

  async storeChunk(userPineconeId, vector) {
    try {
      const index = this.pinecone.index(userPineconeId);
      await index.upsert([vector]);
    } catch (error) {
      console.error('‚ùå Pinecone storage failed:', error.message);
      throw error;
    }
  }
}

module.exports = new SimpleDocumentProcessor();